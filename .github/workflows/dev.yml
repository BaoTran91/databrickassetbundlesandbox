name: Deploy to DEV

concurrency: 1

on:
  workflow_dispatch:

  push:
    branches:
      - dev
    paths:
      - "**/*.yml"
      - "**/*.py"

jobs:
  deploy:
    name: "Deploy bundle"
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - run: pip install uv
      - run: pip install -r requirements.txt

      - name: Configure Databricks Connect (optional)
        run: |
          databricks configure --host 'https://dbc-11d28957-3f38.cloud.databricks.com' --token ${{ secrets.E2_DEMO_WEST_TOKEN }}

      - name: Run unit tests
        run: pytest tests/

      - uses: databricks/setup-cli@main

      - run: databricks bundle deploy --auto-approve
        env:
          DATABRICKS_TOKEN: ${{ secrets.E2_DEMO_WEST_TOKEN }}
          DATABRICKS_BUNDLE_ENV: dev
          SPARK_REMOTE: databricks://DEFAULT

  pipeline_update:
    name: "Run pipeline update"
    runs-on: ubuntu-latest

    needs:
      - deploy

    steps:
      - uses: actions/checkout@v3

#       - uses: actions/setup-python@v4
#         with:
#           python-version: 3.11
#           cache: 'pip'

#       - run: pip install -r requirements.txt
#         working-directory: .github/support

      - uses: databricks/setup-cli@main

      - shell: bash
        name: Run pipeline update
        run: |
          set -o pipefail
          pip install uv
          databricks bundle run datakickstart_dabs_job --refresh-all 2>&1 | tee output.log
        env:
          DATABRICKS_TOKEN: ${{ secrets.E2_DEMO_WEST_TOKEN }}
          DATABRICKS_BUNDLE_ENV: dev

#       - shell: bash
#         if: always()
#         name: Emit annotations
#         run: |
#           pipeline_id=$(head -1 output.log | awk -F / '{print $6}')
#           update_id=$(head -1 output.log | awk -F / '{print $8}')
#           PYTHONPATH=$PWD:$PWD/.github/support python3 .github/support/emit_annotations.py $pipeline_id $update_id
#         env:
#           DATABRICKS_TOKEN: ${{ secrets.E2_DEMO_WEST_TOKEN }}