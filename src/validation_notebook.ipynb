{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee353e42-ff58-4955-9608-12865bd0950e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Validation notebook\n",
    "\n",
    "This notebook is executed using Databricks Workflows as defined in resources/notebook_validation_job.yml. It is used to check summary table for valid results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Frame assert\n",
    "Compare results from test data set against an expected set of values that is generated with simpler logic. This is more dynamic but involves putting more logic into the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bca260b-13d1-448f-8082-30b60a85c9ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.testing.utils import assertDataFrameEqual\n",
    "\n",
    "result_counts = spark.sql(\"\"\"\n",
    " SELECT count(distinct artist_name) artists, count(1) rows\n",
    " FROM sandbox.assetbundle_tutorial_dev.top_artists_by_year_copy\n",
    "        \"\"\")\n",
    "\n",
    "expected_counts = spark.sql(\"\"\"\n",
    "        WITH source_agg (\n",
    "            SELECT artist_name, total_number_of_songs, year\n",
    "            FROM sandbox.tutorial.top_artists_by_year\n",
    "            WHERE year >= 1990 ORDER BY total_number_of_songs DESC, year DESC\n",
    "        )\n",
    "        SELECT count(distinct artist_name) artists, count(1) rows\n",
    "        FROM source_agg\n",
    "        \"\"\")\n",
    "\n",
    "assertDataFrameEqual(result_counts, expected_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b174dd1e-3c3d-4933-a4c4-8e5d52500cc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple assert\n",
    "Option you can use if counts will stay consistent in the test environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "        SELECT count(1) rows\n",
    "        FROM main.datakickstart_dev.trip_summary\n",
    "        \"\"\").first()\n",
    "\n",
    "# Option 1\n",
    "assert result.rows == 11921\n",
    "\n",
    "# Option 2\n",
    "expected_counts = Row(rows=11921)\n",
    "assert result == expected_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No errors detected\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
